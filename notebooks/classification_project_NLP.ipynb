{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/dianaterraza/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dianaterraza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dianaterraza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dianaterraza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load & Prepare Dataset\n",
    "\n",
    "The Sentence Polarity dataset contains 5,331 positive and 5,331 negative sentences. We'll load this dataset and prepare it for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/dianaterraza/.cache/kagglehub/datasets/bricevergnou/spotify-recommendation/versions/2/data.csv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download and get the path to the file\n",
    "path = kagglehub.dataset_download('bricevergnou/spotify-recommendation', path='data.csv')\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  the rock is destined to be the 21st century's ...\n",
      "1  the gorgeously elaborate continuation of \" the...\n",
      "2                     effective but too-tepid biopic\n",
      "3  if you sometimes like to go to the movies to h...\n",
      "4  emerges as something rare , an issue movie tha...\n"
     ]
    }
   ],
   "source": [
    "# Read the positive and negative sentiment files\n",
    "df_sent_pos = pd.read_csv('/Users/dianaterraza/Desktop/NLP/Data/sentence_polarity 2/rt-polarity.pos', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "df_sent_neg = pd.read_csv('/Users/dianaterraza/Desktop/NLP/Data/sentence_polarity 2/rt-polarity.neg', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "\n",
    "# Display the first few rows of the positive dataset to understand its structure\n",
    "print(df_sent_pos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to 'sentence'\n",
    "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)\n",
    "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inplace=True ensures the changes are applied directly to the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Converts text to lowercase.\n",
    "2. Removes punctuation using regular expressions.\n",
    "3. Removes extra whitespace.\n",
    "4. Tokenizes sentences into words.\n",
    "5. Removes stop words.\n",
    "6. Lemmatizes words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(sentences):\n",
    "    # Convert all tokens to lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove extra whitespaces between words\n",
    "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each preprocessing step cleans the text data and prepares it for model training.\n",
    "* Stop words like \"the\", \"is\", and \"and\" are removed to focus on meaningful words.\n",
    "* Lemmatization reduces words to their base form (e.g., \"running\" → \"run\").\n",
    "* Note what we pass after the return keyword! [' '.join(sentence) for sentence in lemmatized_sentences] does an important step for us: it collects separate tokens back to a string. Here is a concrete example: we move from ['simplistic', 'silly', 'tedious'] to `\n",
    "simplistic silly tedious. In other words, we move from a list of tokens to a single sentence in a form of a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic silly tedious\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sentences\n",
    "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
    "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
    "\n",
    "# Print the first preprocessed negative sentence\n",
    "print(neg_preprocessed_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Combine Datasets\n",
    "\n",
    "We merge the positive and negative sentences into a single list called sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed positive and negative sentences\n",
    "sentences = pos_preprocessed_sentences + neg_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create Labels\n",
    "\n",
    "Labels (also called targets) distinguish positive and negative sentences. Positive sentences are labeled as 1, and negative ones as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for all labels\n",
    "polarities = []\n",
    "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
    "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the polarities list matches the sentences list, maintaining the correct label for each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences and labels into a single list\n",
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split the shuffled list back into sentences and labels\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Split Dataset\n",
    "\n",
    "We’ll split the data into training and test sets, using 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test set: 2133\n"
     ]
    }
   ],
   "source": [
    "# Define train-test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training set\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "# Print sizes of training and test sets\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vectorizing Texts, Training Models & Evaluating Their Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Transforming Text into Features\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency \n",
    "\n",
    "The TF-IDF Vectorizer is a widely used tool for this purpose. It transforms sentences into a sparse matrix where each row corresponds to a document (sentence) and each column represents a term (word or token)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we’ll use the TfidfVectorizer out of the box with its default configuration. By default, this includes:\n",
    "* Only unigrams (individual terms/tokens) as features.\n",
    "* A maximum document frequency of 1.0 (no terms are excluded based on frequency).\n",
    "* Normalization applied to the resulting feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples: 8529, #Features: 16490\n"
     ]
    }
   ],
   "source": [
    "# Import TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the vectorizer with default parameters\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the training data into a TF-IDF matrix\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Check the number of samples and features\n",
    "num_samples, num_features = X_train_tfidf.shape\n",
    "print(\"#Samples: {}, #Features: {}\".format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a simple yet effective algorithm for binary classification tasks, such as predicting sentiment polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluating the Classifier\n",
    "\n",
    "After training the model, we need to assess its performance on unseen test data. Evaluation involves transforming the test data into the same TF-IDF format as the training data, making predictions, and calculating key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Transform Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule here is that .fit_transform() is always used for the training data and .transform() for any other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data into TF-IDF format\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict polarities for the test data\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Generate Evaluation Report\n",
    "\n",
    "We use the classification_report() function to generate precision, recall, and F1-score metrics for each class, as well as the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76      1055\n",
      "           1       0.77      0.74      0.76      1078\n",
      "\n",
      "    accuracy                           0.76      2133\n",
      "   macro avg       0.76      0.76      0.76      2133\n",
      "weighted avg       0.76      0.76      0.76      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate and display the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interpreting the Results\n",
    "Let’s break down the key metrics:\n",
    "* Precision: How many of the model’s positive predictions were correct? For example, the precision for class 0 (negative sentiment) is 75%, meaning 75% of the sentences predicted as negative were actually negative.\n",
    "\n",
    "* Recall: How many of the actual positive cases were correctly identified? For class 1 (positive sentiment), the recall is 75%, meaning the model correctly identified 75% of all positive sentences.\n",
    "\n",
    "* F1-score: This is the harmonic mean of precision and recall, providing a balanced measure. In this example, both classes have an F1-score of 0.75.\n",
    "\n",
    "**Is 75% a Good F1-Score?**\n",
    "A random classifier for a binary task would achieve an F1-score of ~50%.\n",
    "Our model’s F1-score of 75% shows that it performs significantly better than random guessing. However, it’s not perfect, indicating room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing K-Fold Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores for each fold: [0.75837321 0.75638051 0.73882353 0.74340528 0.72941176 0.77351916\n",
      " 0.75059382 0.76091954 0.75919336 0.75417661]\n",
      "F1 Score (Mean/Average): 0.752\n",
      "F1 Score (Standard Deviation): 0.012\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Perform 10-fold cross-validation on the training data\n",
    "f1_scores_list = cross_val_score(\n",
    "    LogisticRegression(),            # Model: Logistic Regression\n",
    "    X_train_tfidf,                   # Features: TF-IDF transformed training data\n",
    "    y_train,                         # Labels: Training labels\n",
    "    cv=10,                           # Number of folds\n",
    "    scoring=\"f1\"                     # Evaluation metric: F1 score\n",
    ")\n",
    "\n",
    "# Display the F1 scores for each fold\n",
    "print(f\"F1 Scores for each fold: {f1_scores_list}\")\n",
    "\n",
    "# Calculate and display the mean and standard deviation of the F1 scores\n",
    "print(\"F1 Score (Mean/Average): {:.3f}\".format(f1_scores_list.mean()))\n",
    "print(\"F1 Score (Standard Deviation): {:.3f}\".format(f1_scores_list.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "*1. F1 Scores Across Folds:*\n",
    "* These represent the F1 scores obtained for each fold. In this example, the scores ranges from around 74 % to 77 %. They are fairly consistent, which is a good sign.\n",
    "\n",
    "*2. Mean F1 Score:*\n",
    "* The average F1 score is a reliable measure of the model’s overall performance.\n",
    "\n",
    "*3. Standard Deviation:*\n",
    "* A low standard deviation indicates that the model performs consistently across different subsets of the data.\n",
    "* A high standard deviation might indicate issues with data shuffling or an insufficient dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points to Remember\n",
    "* Use .fit_transform() only on the training data (or k-1 folds during cross-validation). For the test data or validation folds, always use .transform() to avoid data leakage.\n",
    "* The test data is not involved in cross-validation. It remains unseen until final model evaluation.\n",
    "* Choose the value of k based on dataset size and computational resources. Common choices are k=5 or k=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perform Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Hands-On Example of Hyperparameter Tuning\n",
    "In this lesson, we'll perform a simple hyperparameter tuning exercise to identify the best combination of:\n",
    "\n",
    "1. The classification algorithm (Logistic Regression VS LinearSVC)\n",
    "2. The maximum size of n-grams (from 1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up the Experiment:\n",
    "We need to test every possible combination of our selected hyperparameters to identify which one yields the best F1-score during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LinearSVC, n-gram size: 1 => F1-score: 0.742\n",
      "Classifier: LinearSVC, n-gram size: 2 => F1-score: 0.755\n",
      "Classifier: LinearSVC, n-gram size: 3 => F1-score: 0.755\n",
      "Classifier: LinearSVC, n-gram size: 4 => F1-score: 0.754\n",
      "Classifier: LogisticRegression, n-gram size: 1 => F1-score: 0.753\n",
      "Classifier: LogisticRegression, n-gram size: 2 => F1-score: 0.747\n",
      "Classifier: LogisticRegression, n-gram size: 3 => F1-score: 0.741\n",
      "Classifier: LogisticRegression, n-gram size: 4 => F1-score: 0.738\n",
      "\n",
      "Best Configuration:\n",
      "Classifier: LinearSVC, Max n-gram size: 2, F1-score: 0.755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize placeholders to store the best configuration\n",
    "best_score = -1.0\n",
    "best_classifier = None\n",
    "best_ngram_size = -1\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "classifiers = [LinearSVC(), LogisticRegression(solver=\"sag\")]\n",
    "ngram_sizes = [1, 2, 3, 4]\n",
    "\n",
    "# Loop through all combinations of classifiers and n-gram sizes\n",
    "for classifier in classifiers:\n",
    "    for n in ngram_sizes:\n",
    "        # Define the vectorizer with the current n-gram size\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, n))\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)  # Transform training data\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        f1_scores = cross_val_score(classifier, X_train_tfidf, y_train, cv=10, scoring='f1')\n",
    "        avg_f1_score = f1_scores.mean()  # Calculate average F1-score\n",
    "\n",
    "        # Print the result for this combination\n",
    "        print(f\"Classifier: {type(classifier).__name__}, n-gram size: {n} => F1-score: {avg_f1_score:.3f}\")\n",
    "\n",
    "        # Save the best configuration\n",
    "        if avg_f1_score > best_score:\n",
    "            best_score = avg_f1_score\n",
    "            best_classifier = classifier\n",
    "            best_ngram_size = n\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"Classifier: {type(best_classifier).__name__}, Max n-gram size: {best_ngram_size}, F1-score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearSVC Model (Support Vector Classifier with a Linear Kernel)**\n",
    "\n",
    "LinearSVC is a Support Vector Machine (SVM) model that uses a linear kernel to classify data.\n",
    "\n",
    "It is useful for high-dimensional datasets, such as text classification (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Training the Best Model\n",
    "After identifying the best combination of parameters, we can train the final model on the entire training dataset and evaluate it using the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      1055\n",
      "           1       0.79      0.74      0.76      1078\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n",
      "Accuracy: 0.769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Use the best configuration to train the final model\n",
    "final_vectorizer = TfidfVectorizer(ngram_range=(1, best_ngram_size))\n",
    "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = final_vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate and display results\n",
    "print(\"\\nFinal Model Results:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
